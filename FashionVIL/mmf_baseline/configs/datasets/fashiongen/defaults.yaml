# dataset_config:
#   fashiongen:
#       data_dir: ${env.data_dir}/datasets
#       use_images: true
#       use_features: false
#       use_patch_labels: false
#       images:
#         train:
#         - FashionGen/train
#         val:
#         - FashionGen/val
#         test:
#         - FashionGen/val
#       annotations:
#         train:
#         - FashionGen/train_info.json
#         val:
#         - FashionGen/val_info.json
#         test:
#         - FashionGen/val_info.json
#       features:
#         train:
#         - SFashionGen/train_features
#         val:
#         - FashionGen/val_features
#         test:
#         - FashionGen/val_features
#       processors:
#         text_processor:
#           type: bert_tokenizer
#           params:
#             tokenizer_config:
#               type: bert-base-uncased
#               params:
#                 do_lower_case: true
#             mask_probability: 0
#             max_seq_length: 75
#         train_image_processor:
#           type: torchvision_transforms
#           params:
#             transforms:
#               - type: Resize
#                 params:
#                   size: [256, 256]
#               - type: RandomCrop
#                 params:
#                   size: [224, 224]
#               - RandomHorizontalFlip
#               - ToTensor
#               - type: Normalize
#                 params:
#                   mean: [0.46777044, 0.44531429, 0.40661017]
#                   std: [0.12221994, 0.12145835, 0.14380469]
#         eval_image_processor:
#           type: torchvision_transforms
#           params:
#             transforms:
#               - type: Resize
#                 params:
#                   size: [256, 256]
#               - type: CenterCrop
#                 params:
#                   size: [224, 224]
#               - ToTensor
#               - type: Normalize
#                 params:
#                   mean: [0.46777044, 0.44531429, 0.40661017]
#                   std: [0.12221994, 0.12145835, 0.14380469]


dataset_config:
  fashiongen:
      data_dir: ${env.data_dir}/datasets
      use_images: true
      use_features: false
      use_patch_labels: false
      images:
        train:
        - Sub-FACAD-drop07/train # FashionGen/train
        val:
        - Sub-FACAD-drop07/val # FashionGen/val
        test:
        - Sub-FACAD-drop07/val # FashionGen/val
      annotations:
        train:
        - Sub-FACAD-drop07/train_info.json # FashionGen/train_info.json
        val:
        - Sub-FACAD-drop07/val_info.json # FashionGen/val_info.json
        test:
        - Sub-FACAD-drop07/val_info.json # FashionGen/val_info.json
      features:
        train:
        - Sub-FACAD-drop07/train_features # FashionGen/train_features
        val:
        - Sub-FACAD-drop07/val_features # FashionGen/val_features
        test:
        - Sub-FACAD-drop07/val_features # FashionGen/val_features
      processors:
        text_processor:
          type: bert_tokenizer
          params:
            tokenizer_config:
              type: bert-base-uncased
              params:
                do_lower_case: true
            mask_probability: 0
            max_seq_length: 75
        train_image_processor:
          type: torchvision_transforms
          params:
            transforms:
              - type: Resize
                params:
                  size: [256, 256] # [330, 506]
              - type: RandomCrop
                params:
                  size: [224, 224]
              - RandomHorizontalFlip
              - ToTensor
              - type: Normalize
                params:
                  mean: [0.46777044, 0.44531429, 0.40661017]
                  std: [0.12221994, 0.12145835, 0.14380469]
        eval_image_processor:
          type: torchvision_transforms
          params:
            transforms:
              - type: Resize
                params:
                  size: [256, 256] # [330, 506]
              - type: CenterCrop
                params:
                  size: [224, 224]
              - ToTensor
              - type: Normalize
                params:
                  mean: [0.46777044, 0.44531429, 0.40661017]
                  std: [0.12221994, 0.12145835, 0.14380469]



# dataset_config:
#   fashiongen:
#       data_dir: ${env.data_dir}/datasets
#       use_images: true
#       use_features: false
#       use_patch_labels: false
#       images:
#         train:
#         - Sub-FACAD-Double/train # FashionGen/train
#         val:
#         - Sub-FACAD-Double/val # FashionGen/val
#         test:
#         - Sub-FACAD-Double/val # FashionGen/val
#       annotations:
#         train:
#         - Sub-FACAD-Double/train_info.json # FashionGen/train_info.json
#         val:
#         - Sub-FACAD-Double/val_info.json # FashionGen/val_info.json
#         test:
#         - Sub-FACAD-Double/val_info.json # FashionGen/val_info.json
#       features:
#         train:
#         - Sub-FACAD-Double/train_features # FashionGen/train_features
#         val:
#         - Sub-FACAD-Double/val_features # FashionGen/val_features
#         test:
#         - Sub-FACAD-Double/val_features # FashionGen/val_features
#       processors:
#         text_processor:
#           type: bert_tokenizer
#           params:
#             tokenizer_config:
#               type: bert-base-uncased
#               params:
#                 do_lower_case: true
#             mask_probability: 0
#             max_seq_length: 75
#         train_image_processor:
#           type: torchvision_transforms
#           params:
#             transforms:
#               - type: Resize
#                 params:
#                   size: [256, 256]
#               - type: RandomCrop
#                 params:
#                   size: [224, 224]
#               - RandomHorizontalFlip
#               - ToTensor
#               - type: Normalize
#                 params:
#                   mean: [0.46777044, 0.44531429, 0.40661017]
#                   std: [0.12221994, 0.12145835, 0.14380469]
#         eval_image_processor:
#           type: torchvision_transforms
#           params:
#             transforms:
#               - type: Resize
#                 params:
#                   size: [256, 256]
#               - type: CenterCrop
#                 params:
#                   size: [224, 224]
#               - ToTensor
#               - type: Normalize
#                 params:
#                   mean: [0.46777044, 0.44531429, 0.40661017]
#                   std: [0.12221994, 0.12145835, 0.14380469]
